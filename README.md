# Distributed-Training-with-Pipeline-Parallelism
Compares the performance of GPipe, 1F1B, and Interleaved 1F1B using distributed CPU training (gloo backend in PyTorch)

The ipynb notebook runs the experiments and analyzes the resulting data 
The .py contains the main logic that is run on each node/CPU core
